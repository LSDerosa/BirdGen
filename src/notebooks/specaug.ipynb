{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as _T\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dataloaders.rawaudiodataset import RawAudioDataset\n",
    "from dataloaders.audiodataset  import AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  # power_spec = librosa.power_to_db(spec)\n",
    "  power_spec = spec\n",
    "  im = axs.imshow(power_spec, origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def get_spectrogram(waveform,*args, **kwargs):\n",
    "  # spectrogram = T.Spectrogram(\n",
    "  #     *args, **kwargs\n",
    "  # )\n",
    "  # mel_scale = T.MelScale(\n",
    "  #           n_mels=256, sample_rate=22050, n_stft=1024// 2 + 1)\n",
    "\n",
    "  # return mel_scale(spectrogram(waveform))\n",
    "\n",
    "\n",
    "\n",
    "  mel_spectrogram = T.MelSpectrogram(\n",
    "     *args, **kwargs,\n",
    "      # center=True,\n",
    "      # pad_mode=\"reflect\",\n",
    "      # power=2.0,\n",
    "      norm=\"slaney\",\n",
    "      onesided=True,\n",
    "      n_mels=256,\n",
    "      mel_scale=\"htk\",\n",
    "  )\n",
    "  return mel_spectrogram(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_collate_2(batch_):  #Collate function for Raw Audio\n",
    "    def contains_none(b):\n",
    "        for elem in b:\n",
    "            if elem is None:\n",
    "                return False\n",
    "        return True\n",
    "    batch = list(filter(lambda x: contains_none(x), batch_))\n",
    "    input = torch.stack([b[0] for b in batch])\n",
    "    label = torch.stack([b[1] for b in batch])\n",
    "    if len(batch[0]) == 2:\n",
    "        return input, label\n",
    "    filepath = [b[2] for b in batch]\n",
    "    return input, label, filepath\n",
    "\n",
    "\n",
    "def _custom_collate(batch_):\n",
    "    def contains_none(b):\n",
    "        for elem in b:\n",
    "            if elem is None:\n",
    "                return False\n",
    "        return True\n",
    "    # print(batch_)\n",
    "    batch = list(filter(lambda x: contains_none(x), batch_))\n",
    "    input = torch.stack([b[0] for b in batch])\n",
    "    label = torch.stack([b[1] for b in batch])\n",
    "    if len(batch[0]) == 2:\n",
    "        return input, label\n",
    "    filepath = [b[2] for b in batch]\n",
    "    return input, label, filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample_file = glob.glob(\"/Users/test/Documents/Projects/Master/birds-generation/data/cleaned_train/Erirub_song/*.wav\")[0]\n",
    "# sample_file = glob.glob(\"/Users/test/Documents/Projects/Master/ArticBirdsSounds/audio_annots/cleaned_train/Gull/*.wav\")[3]\n",
    "# # audio_spec = np.load(sample_file)\n",
    "\n",
    "# audio_file =  os.path.splitext(sample_file)[0]+\".wav\"\n",
    "# print(audio_file)\n",
    "# assert os.path.isfile(audio_file), (audio_file, sample_file)\n",
    "# # audio, sr = torchaudio.load(audio_file)\n",
    "# audio, sr = AudioDataset._get_sample(path=audio_file, resample=22050)\n",
    "# spec = get_spectrogram(audio, n_fft=1024, win_length=None, hop_length=256, center=True, pad_mode=\"reflect\", power=1.0)\n",
    "# plot_spectrogram(spec[0], title=\"Original\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1., p=0.5):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        if np.random.rand() > self.p:\n",
    "            return tensor\n",
    "        else:\n",
    "            return tensor + torch.randn(tensor.size()) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "def custom_augment_torchaudio(input, transforms):\n",
    "    image = deepcopy(input)\n",
    "    ops = [\n",
    "        ]\n",
    "    for transform in transforms:\n",
    "        transform = transform.lower()\n",
    "        if \"masking\" in transform:\n",
    "            max_mask_size = 7\n",
    "            ops.append(T.FrequencyMasking(freq_mask_param=max_mask_size))\n",
    "            ops.append(T.TimeMasking(time_mask_param=max_mask_size))\n",
    "        elif transform == \"input_dropout\":\n",
    "            ops.append(_T.RandomErasing(p=1.0, scale=(0.01, 0.1), inplace=False))\n",
    "        elif transform == \"spec_stretching\":\n",
    "            ops.append(T.TimeStretch(n_freq=128, fixed_rate=0.5))\n",
    "    \n",
    "    # ops.append( _T.Resize(size=(512, 64)))\n",
    "    print(ops)\n",
    "    out = {\"image\": _T.Compose(ops)(image)}\n",
    "    return out\n",
    "\n",
    "def custom_augment(input, transforms):\n",
    "\n",
    "    image = deepcopy(input)\n",
    "    # image = image[None]\n",
    "    \n",
    "    if isinstance(transforms, _T.Compose):\n",
    "        out = {\"image\": transforms(image)}\n",
    "        return out\n",
    "\n",
    "    for transform in transforms:\n",
    "        transform = transform.lower()\n",
    "        print(\"Before\", image.shape)\n",
    "        # remove a portion of 20 pixels at random position in the image along the frequency axis using numpy slicing\n",
    "\n",
    "        max_mask_size = 7\n",
    "        if transform == \"freqmasking\":\n",
    "            mazk_size = np.random.randint(0, max_mask_size)\n",
    "            start = np.random.randint(0, image.shape[1] -mazk_size )\n",
    "            image[:, start:start +mazk_size , :] = 0 # Image shape is (1, 256, 32)\n",
    "        elif transform == \"timemasking\":\n",
    "            mazk_size = np.random.randint(0, max_mask_size)\n",
    "            start = np.random.randint(0, image.shape[2] -mazk_size )\n",
    "            image[:, :, start:start +mazk_size ] = 0\n",
    "        elif transform == \"masking\":\n",
    "            mazk_size = np.random.randint(0, max_mask_size)\n",
    "            start = np.random.randint(0, image.shape[1] -mazk_size )\n",
    "            image[:, start:start +mazk_size , :] = 0\n",
    "            mazk_size = np.random.randint(0, max_mask_size)\n",
    "            start = np.random.randint(0, image.shape[2] -mazk_size )\n",
    "            image[:, :, start:start +mazk_size ] = 0\n",
    "\n",
    "\n",
    "        elif transform == \"input_dropout\":\n",
    "            ## perform random 10% erasing on the x axis of the spectrogram using numpy.\n",
    "            mask_size = np.random.randint(0, int(image.shape[-1] * 0.1))\n",
    "            start = np.random.randint(0, image.shape[-1] - mask_size)\n",
    "            image[:, :,start:start + mask_size] = 0\n",
    "        elif transform == \"spec_stretching\":\n",
    "            #resize the image along the time axis using bilinear interpolation with a random scale factor between 0.8 and 1.2 using OpenCV\n",
    "            scale_factor = np.random.uniform(0.8, 1.2)\n",
    "            image = cv2.resize(image[0], (int(image.shape[-1] * scale_factor), image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "            image = image[None]\n",
    "        elif transform == \"input_noise\":\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "            print(\"After\", image.shape)\n",
    "    out = {\"image\":image}\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in the dataset:  ['Turphi_song', 'Alaarv_song', 'Galcri_call', 'Parate_song', 'Erirub_song', 'Cetcet_song', 'Serser_song', 'Sylcan_song', 'Turmer_call', 'Sylcan_call']\n",
      "Overriding spec variables because use_spectrogram is true\n",
      "Data initialization\n",
      "All paths 177\n",
      "Loaded data from cache 177 177\n",
      "Before torch.Size([1, 256, 32])\n",
      "Error Expected Ptr<cv::UMat> for argument 'src' on file: ../../data/cleaned_train/Alaarv_song/nips4b_birds_trainfile583-8f0714.wav -- Traceback (most recent call last):\n",
      "  File \"../dataloaders/audiodataset.py\", line 260, in __getitem__\n",
      "    features = self.transforms(input=features)['image']\n",
      "  File \"/var/folders/1z/2btfs6ws4mz5j62xpq_mfhk80000gn/T/ipykernel_39291/3735482431.py\", line 76, in custom_augment\n",
      "    image = cv2.resize(image[0], (int(image.shape[-1] * scale_factor), image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
      "TypeError: Expected Ptr<cv::UMat> for argument 'src'\n",
      "\n",
      "Before torch.Size([1, 256, 32])\n",
      "Error Expected Ptr<cv::UMat> for argument 'src' on file: ../../data/cleaned_train/Sylcan_call/nips4b_birds_trainfile208-70e306.wav -- Traceback (most recent call last):\n",
      "  File \"../dataloaders/audiodataset.py\", line 260, in __getitem__\n",
      "    features = self.transforms(input=features)['image']\n",
      "  File \"/var/folders/1z/2btfs6ws4mz5j62xpq_mfhk80000gn/T/ipykernel_39291/3735482431.py\", line 76, in custom_augment\n",
      "    image = cv2.resize(image[0], (int(image.shape[-1] * scale_factor), image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
      "TypeError: Expected Ptr<cv::UMat> for argument 'src'\n",
      "\n",
      "Before torch.Size([1, 256, 32])\n",
      "Error Expected Ptr<cv::UMat> for argument 'src' on file: ../../data/cleaned_train/Erirub_song/nips4b_birds_trainfile321-f319b4.wav -- Traceback (most recent call last):\n",
      "  File \"../dataloaders/audiodataset.py\", line 260, in __getitem__\n",
      "    features = self.transforms(input=features)['image']\n",
      "  File \"/var/folders/1z/2btfs6ws4mz5j62xpq_mfhk80000gn/T/ipykernel_39291/3735482431.py\", line 76, in custom_augment\n",
      "    image = cv2.resize(image[0], (int(image.shape[-1] * scale_factor), image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
      "TypeError: Expected Ptr<cv::UMat> for argument 'src'\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1z/2btfs6ws4mz5j62xpq_mfhk80000gn/T/ipykernel_39291/1460850931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_custom_collate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scologan/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scologan/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/scologan/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/1z/2btfs6ws4mz5j62xpq_mfhk80000gn/T/ipykernel_39291/3756070761.py\u001b[0m in \u001b[0;36m_custom_collate\u001b[0;34m(batch_)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# print(batch_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "#Instantiate an object of the class rawaudiodataset\n",
    "classes = os.listdir(\"../../data/cleaned_train\") \n",
    "#remove .DS_Store from the list of classes\n",
    "classes = [x for x in classes if x != \".DS_Store\"]\n",
    "print(\"Classes in the dataset: \", classes)\n",
    "\n",
    "# transform = T.FrequencyMasking(freq_mask_param=80)\n",
    "# transforms = _T.Compose(\n",
    "#     [\n",
    "#         _T.Resize(size=(512, 64)),\n",
    "#         T.FrequencyMasking(freq_mask_param=10),\n",
    "#         T.TimeMasking(time_mask_param=10),\n",
    "#         # T.TimeStretch(n_freq=512, fixed_rate=0.5),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "transforms = ['spec_stretching', 'masking', 'input_dropout']\n",
    "transform = partial(custom_augment, transforms=transforms)\n",
    "#Create a dataset object\n",
    "batch_size = 3\n",
    "# dataset = RawAudioDataset(data_path=\"../../data/cleaned_train.txt\", root_dir=\"../../data/\", classes_name=classes, sr=16000, window_length=16384, use_spectrogram=True, transform=transform)\n",
    "dataset = AudioDataset(data_path=\"../../data/cleaned_train.txt\", root_dir=\"../../data/\", classes_name=classes, sr=22050, window_length=16384, use_spectrogram=True, transforms=transform, return_tuple=True, return_tuple_of3=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=_custom_collate)\n",
    "\n",
    "for i, sample in enumerate(dataloader):\n",
    "  spec = sample[0]\n",
    "  for j in range(batch_size):\n",
    "    spec_temp = spec.squeeze(1)[j]\n",
    "    print(spec_temp.min(), spec_temp.max())\n",
    "    plot_spectrogram(spec_temp, title=\"Original\")\n",
    "  if i > 5:\n",
    "    break\n",
    "# del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_sample(path, resample=None):\n",
    "#     effects = [[\"remix\", \"1\"]]\n",
    "#     if resample:\n",
    "#         effects.extend(\n",
    "#             [\n",
    "#                 [\"lowpass\", f\"{resample // 2}\"],\n",
    "#                 [\"rate\", f\"{resample}\"],\n",
    "#             ]\n",
    "#         )\n",
    "#     return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "\n",
    "# def get_speech_sample(audio_path, resample=None):\n",
    "#     return _get_sample(audio_path, resample=resample)\n",
    "# def plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None):\n",
    "#     fig, axs = plt.subplots(1, 1)\n",
    "#     axs.set_title(title or \"Spectrogram (db)\")\n",
    "#     axs.set_ylabel(ylabel)\n",
    "#     axs.set_xlabel(\"frame\")\n",
    "#     im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n",
    "#     if xmax:\n",
    "#         axs.set_xlim((0, xmax))\n",
    "#     fig.colorbar(im, ax=axs)\n",
    "#     plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_file = \"../../data/cleaned_train/Sylcan_song/nips4b_birds_trainfile034-42e299.wav\"\n",
    "# waveform, sr = get_speech_sample(audio_file, resample=16384)\n",
    "# spectrogram = T.Spectrogram(\n",
    "#     n_fft=1024,\n",
    "#     win_length=1024,\n",
    "#     hop_length=256,\n",
    "#     center=True,\n",
    "#     pad_mode=\"reflect\",\n",
    "#     power=2.0,\n",
    "# )\n",
    "# spec = spectrogram(waveform)\n",
    "# print(spec.shape)\n",
    "# plot_spectrogram(spec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, file in enumerate(glob.glob(\"/Users/test/Documents/Projects/Master/ArticBirdsSounds/audio_annots/augmented/interpolation/Gull/*.npy\")):\n",
    "#     audio = np.load(file)\n",
    "#     print(audio.shape)\n",
    "#     plt.imshow(audio)\n",
    "#     plt.show()\n",
    "#     if i > 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('scologan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e93dd7469fb6698e09fbfba73e5ce40dc1dc5e356aab3eb579a371dc5e93993c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
